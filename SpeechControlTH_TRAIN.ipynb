{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Audio Recognition using Tensorflow\n",
    "### Speech Recognition for Controlling Robot (THAI COMMAND)\n",
    "#### By. Arunwat Moonbung\n",
    "#### SPECIAL THANKS TO \"Leandro Roser\"\n",
    "#### FOR AUDIO-PREPROCESSING e.g. AUDIO-AUGMENTATION TECHNIQUES, DATA LOADING, DATA INTEGRITY OBSERVE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import IPython.display as ipd\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import librosa, librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,LabelBinarizer\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.python.client import device_lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using Tensorflow 2.8.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0,1'\n",
    "print(f\"Currently using Tensorflow {tf.__version__}\")\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())\n",
    "tf.random.set_seed(6131501066)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASS AND FUNCTION DEFINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join(\"Data_Thai\",\"train\")\n",
    "DATASET_JSON = os.path.join(\"Data_Thai\",\"classmap.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_audio(signal, NUM_SAMPLES_TO_CONSIDER):\n",
    "    if len(signal) >= NUM_SAMPLES_TO_CONSIDER:\n",
    "        return signal[:NUM_SAMPLES_TO_CONSIDER]\n",
    "    else:\n",
    "        return np.pad(signal, pad_width=(NUM_SAMPLES_TO_CONSIDER - len(signal), 0), mode='constant', constant_values=(0, 0))\n",
    "    \n",
    "def pad_audio_sec(self, signal, DURATION, NUM_SAMPLES_TO_CONSIDER):\n",
    "        TOTAL_SAMPLE = DURATION*NUM_SAMPLES_TO_CONSIDER\n",
    "        if len(signal) >= TOTAL_SAMPLE:\n",
    "            return signal[:TOTAL_SAMPLE]\n",
    "        else:\n",
    "            #return np.pad(signal, pad_width=(0, TOTAL_SAMPLE - len(signal)), mode='constant', constant_values=(0, 0)) # PAD หลัง\n",
    "            return np.pad(signal, pad_width=(TOTAL_SAMPLE - len(signal), 0), mode='constant', constant_values=(0, 0)) # PAD หน้า\n",
    "\n",
    "def chop_audio(signal, NUM_SAMPLES_TO_CONSIDER=16000):\n",
    "    while True:\n",
    "        beg = np.random.randint(0, len(signal) - NUM_SAMPLES_TO_CONSIDER)\n",
    "        yield signal[beg: beg + NUM_SAMPLES_TO_CONSIDER]\n",
    "\n",
    "def choose_background_generator(signal, backgrounds, max_alpha=0.7):\n",
    "    if backgrounds is None:\n",
    "        return signal\n",
    "    my_gen = backgrounds[np.random.randint(len(backgrounds))]\n",
    "    background = next(my_gen) * np.random.uniform(0, max_alpha)\n",
    "    augmented_data = signal + background\n",
    "    augmented_data = augmented_data.astype(type(signal[0]))\n",
    "    return augmented_data\n",
    "\n",
    "def random_shift(signal, NUM_SAMPLES_TO_CONSIDER=16000, shift_max=0.2):\n",
    "    shift = np.random.randint(NUM_SAMPLES_TO_CONSIDER * shift_max)\n",
    "    out = np.roll(signal, shift)\n",
    "    # Time shift\n",
    "    if shift > 0:\n",
    "        out[:shift] = 0\n",
    "    else:\n",
    "        out[shift:] = 0\n",
    "    return out\n",
    "\n",
    "def random_change_pitch(signal, NUM_SAMPLES_TO_CONSIDER=16000):\n",
    "    pitch_factor = np.random.randint(1, 4)\n",
    "    out = librosa.effects.pitch_shift(y=signal, sr=NUM_SAMPLES_TO_CONSIDER, n_steps=pitch_factor)\n",
    "    return out\n",
    "\n",
    "def random_speed_up(signal):\n",
    "    where = [\"start\", \"end\"][np.random.randint(0, 1)]\n",
    "    speed_factor = np.random.uniform(0, 0.5)\n",
    "    up = librosa.effects.time_stretch(y=signal, rate=1 + speed_factor)\n",
    "    up_len = up.shape[0]\n",
    "    if where == \"end\":\n",
    "        up = np.concatenate((up, np.zeros((signal.shape[0] - up_len,))))\n",
    "    else:\n",
    "        up = np.concatenate((np.zeros((signal.shape[0] - up_len,)), up))\n",
    "    return up\n",
    "\n",
    "def get_image_list(train_audio_path):\n",
    "    classes = os.listdir(train_audio_path)\n",
    "    classes = [thisclass for thisclass in classes if thisclass != '_background_noise_']\n",
    "    index = [i for i,j in enumerate(classes)]\n",
    "    outlist = []\n",
    "    labels = []\n",
    "    text_labels = dict(zip(classes, index))\n",
    "    for thisindex, thisclass in zip(index, classes):\n",
    "        filelist = [f for f in os.listdir(os.path.join(train_audio_path, thisclass)) if f.endswith('.wav')]\n",
    "        filelist = [os.path.join(train_audio_path, thisclass, x) for x in filelist]\n",
    "        outlist.append(filelist)\n",
    "        labels.append(np.full(len(filelist), fill_value=thisindex))\n",
    "    try:\n",
    "        with open(DATASET_JSON, \"w\") as f:\n",
    "            json.dump(text_labels, f, indent=4)\n",
    "        print(f\"#: SAVING CLASS LABEL MAPPING.. AT {train_audio_path}\")\n",
    "    except:\n",
    "        print(\"!: ERROR WHILE SAVING .json file\")\n",
    "        \n",
    "    return outlist, labels, text_labels\n",
    "\n",
    "def split_train_test_stratified_shuffle(images_list, labels, train_size=0.7):\n",
    "    classes_size = [len(x) for x in images_list]\n",
    "    classes_vector = [np.arange(x) for x in classes_size]\n",
    "    total = np.sum(classes_size)\n",
    "    total_train = [int(train_size * total * x) for x in classes_size / total]\n",
    "    train_index = [np.random.choice(x, y, replace=False) for x,y in zip(classes_size, total_train)]\n",
    "    validation_index = [np.setdiff1d(i,j) for i,j in zip(classes_vector,train_index)]\n",
    "    train_set = [np.array(x)[idx] for x,idx in zip(images_list, train_index)]\n",
    "    validation_set = [np.array(x)[idx] for x,idx in zip(images_list, validation_index)]\n",
    "    train_labels = [np.array(x)[idx] for x,idx in zip(labels,train_index)]\n",
    "    validation_labels = [np.array(x)[idx] for x,idx in zip(labels, validation_index)]\n",
    "    # ----------------------------------------------------------------------------------------- \n",
    "    train_set = np.array([element for array in train_set for element in array])\n",
    "    validation_set = np.array([element for array in validation_set for element in array])\n",
    "    train_labels = np.array([element for array in train_labels for element in array])\n",
    "    validation_labels = np.array([element for array in validation_labels for element in array])\n",
    "    # -----------------------------------------------------------------------------------------\n",
    "    train_shuffle = np.random.permutation(len(train_set))\n",
    "    validation_shuffle = np.random.permutation(len(validation_set))\n",
    "    train_set = train_set[train_shuffle]\n",
    "    validation_set = validation_set[validation_shuffle]\n",
    "    train_labels = train_labels[train_shuffle]\n",
    "    validation_labels = validation_labels[validation_shuffle]\n",
    "    return train_set, train_labels, validation_set, validation_labels\n",
    "\n",
    "def preprocess_data(file_path, background_generator, n_mfcc=40, hop_length=512, n_fft=4096, NUM_SAMPLES_TO_CONSIDER=16000, threshold=0.7):\n",
    "    # Downsample to NUM_SAMPLES_TO_CONSIDER Hz\n",
    "    signal, sr = librosa.load(file_path, sr=NUM_SAMPLES_TO_CONSIDER)\n",
    "    signal = pad_audio(signal, sr)\n",
    "    if np.random.uniform(0, 1) > threshold:\n",
    "        # ADD NOISE TO 30% OF DATA\n",
    "        signal = choose_background_generator(signal, background_generator)\n",
    "    if np.random.uniform(0, 1) > threshold:\n",
    "        signal = random_shift(signal)\n",
    "    if np.random.uniform(0, 1) > threshold:\n",
    "        signal = random_change_pitch(signal)\n",
    "    if np.random.uniform(0, 1) > threshold:\n",
    "        signal = random_speed_up(signal)\n",
    "    MFCCs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=n_mfcc) # Transpose for sklearn\n",
    "    MFCCs = np.moveaxis(MFCCs, 1, 0)\n",
    "    #scaler = MinMaxScaler() # OPTIONAL FOR Scaling\n",
    "    scaler = StandardScaler() \n",
    "    MFCCs_scaled = scaler.fit_transform(MFCCs)\n",
    "    # MFCCs Input Shape -> (NUM_SAMPLE x NUM_MFCC_COEFFICIENT x 1)\n",
    "    return MFCCs_scaled.reshape(MFCCs_scaled.shape[0], MFCCs_scaled.shape[1], 1)\n",
    "\n",
    "class data_generator(keras.utils.Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size, background_generator):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.background_generator = background_generator\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx_from = idx * self.batch_size\n",
    "        idx_to = (idx + 1) * self.batch_size\n",
    "        batch_x = self.x[idx_from:idx_to]\n",
    "        batch_y = self.y[idx_from:idx_to]\n",
    "        x = [preprocess_data(elem, self.background_generator) for elem in batch_x]\n",
    "        y = batch_y\n",
    "        return np.array(x).astype(np.float32), np.array(y).astype(np.float32)\n",
    "        #return np.array(x), np.array(y)\n",
    "    \n",
    "def build_model(num_classes, input_shape):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=input_shape))\n",
    "    model.add(keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding=\"same\"))\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding=\"same\"))\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\"))\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\"))\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    ##############################################################################\n",
    "    #model.add(keras.layers.Dropout(0.25))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(128))\n",
    "    model.add(keras.layers.Activation(\"relu\"))\n",
    "    #model.add(keras.layers.Dropout(0.5))\n",
    "    #model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "    model.add(keras.layers.Dense(num_classes))\n",
    "    model.add(keras.layers.Activation(\"softmax\"))\n",
    "    return model\n",
    "\n",
    "# kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "\n",
    "'''def build_model(num_classes, input_shape):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=input_shape))\n",
    "    model.add(keras.layers.Conv2D(filters=32, kernel_size=(3,3), \n",
    "                                padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Conv2D(filters=64, kernel_size=(3,3),\n",
    "                                padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Conv2D(filters=128, kernel_size=(3,3),\n",
    "                                padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Conv2D(filters=256, kernel_size=(3,3),\n",
    "                                padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Dropout(0.25))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "    return model'''\n",
    "\n",
    "def multiclass_roc(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    all_labels = np.unique(y_test)\n",
    "\n",
    "    for (idx, c_label) in enumerate(all_labels):\n",
    "        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])\n",
    "        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n",
    "    return roc_auc_score(y_test, y_pred, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train_audio_sample = os.path.join(\"Data_Thai\",\"train\",\"backward\",\"b1.wav\")\n",
    "x,sr = librosa.load(train_audio_sample, sr = 16000)\n",
    "x = pad_audio(x, sr)\n",
    "choose_background_generator(x, background_generator)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING DATASET / VERIFY LOADING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA WITH BACKGROUNDS\n",
    "wavfiles = glob.glob(os.path.join(DATASET_PATH, \"_background_noise_/*wav\"))\n",
    "wavfiles = [librosa.load(elem, sr = 16000)[0] for elem in wavfiles]\n",
    "# wavfile คือ array ของไฟล์เสียง\n",
    "background_generator = [chop_audio(x) for x in wavfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#: SAVING CLASS LABEL MAPPING.. AT Data_Thai\\train\n"
     ]
    }
   ],
   "source": [
    "# TRAINING DATA PATHS, \n",
    "# SPLIT TRAIN-TEST VIA STRATIFIED SAMPLING, \n",
    "# CALL A DATA GENERATOR FOR KERAS\n",
    "# LOAD TRAIN\n",
    "images_list, labels, classes_map = get_image_list(DATASET_PATH)\n",
    "train_set, train_labels, validation_set, validation_labels = split_train_test_stratified_shuffle(images_list, labels)\n",
    "train_datagen = data_generator(x_set=train_set, y_set=train_labels, batch_size=32, background_generator=background_generator)\n",
    "validation_datagen = data_generator(x_set=validation_set, y_set=validation_labels, batch_size=32, background_generator=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(train_datagen[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''display(images_list)\n",
    "display(labels)\n",
    "display(classes_map)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK IF TRAINING DATASET ALSO CONTAIN IN VALIDATION DATASET? // ควรเป็น False เพราะต้องไม่มีตัวไหนที่ซ้ำกัน\n",
    "inv_map =  {v: k for k, v in classes_map.items()}\n",
    "any_present=[i in validation_set for i in train_set]\n",
    "np.any(any_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Data_Thai\\\\train\\\\backward\\\\b108.wav',\n",
       "        'Data_Thai\\\\train\\\\forward\\\\f315.wav',\n",
       "        'Data_Thai\\\\train\\\\turnright\\\\r905.wav',\n",
       "        'Data_Thai\\\\train\\\\grab\\\\g205.wav',\n",
       "        'Data_Thai\\\\train\\\\forward\\\\f211.wav',\n",
       "        'Data_Thai\\\\train\\\\stop\\\\s415.wav',\n",
       "        'Data_Thai\\\\train\\\\release\\\\re809.wav',\n",
       "        'Data_Thai\\\\train\\\\release\\\\re806.wav',\n",
       "        'Data_Thai\\\\train\\\\search\\\\se106.wav',\n",
       "        'Data_Thai\\\\train\\\\release\\\\re809.wav'], dtype='<U34'),\n",
       " ['backward',\n",
       "  'forward',\n",
       "  'turnright',\n",
       "  'grab',\n",
       "  'forward',\n",
       "  'stop',\n",
       "  'release',\n",
       "  'release',\n",
       "  'search',\n",
       "  'release'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK MATCHING FILE .WAV FROM (train_set) and LABELS FROM (train_labels) \n",
    "# โฟลเดอร์ไฟล์ ต้องตรงกับ Label ข้างล่างตามลำดับ ไม่งั้นแสดงว่า Data-pre ผิดพลาด\n",
    "test1 = np.random.randint(10, 100, 10)\n",
    "train_set[test1],[inv_map[int(i)] for i in train_labels[test1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           1\n",
      "0  12.590799\n",
      "1  12.590799\n",
      "2  11.864407\n",
      "3  12.590799\n",
      "4  12.106538\n",
      "5  12.348668\n",
      "6  12.832930\n",
      "7  13.075061\n",
      "           1\n",
      "0  12.509144\n",
      "1  12.655450\n",
      "2  11.923921\n",
      "3  12.582297\n",
      "4  12.143380\n",
      "5  12.435991\n",
      "6  12.728603\n",
      "7  13.021214\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1VALIDATION_SET</th>\n",
       "      <th>1ENTIRE_SET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.509144</td>\n",
       "      <td>12.590799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.655450</td>\n",
       "      <td>12.590799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.923921</td>\n",
       "      <td>11.864407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.582297</td>\n",
       "      <td>12.590799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.143380</td>\n",
       "      <td>12.106538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1VALIDATION_SET  1ENTIRE_SET\n",
       "0        12.509144    12.590799\n",
       "1        12.655450    12.590799\n",
       "2        11.923921    11.864407\n",
       "3        12.582297    12.590799\n",
       "4        12.143380    12.106538"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STRATIFIED SAMPLING WORKS\n",
    "# CHECK UNIQUE VALUE IN NUM_CLASSES (unique) [0, 1, ...]\n",
    "# COUNT NUM_SAMPLE IN EACH UNIQUE VALUE (counts) [500, 500, ...]\n",
    "unique, counts = np.unique(validation_labels, return_counts=True)\n",
    "x = dict(zip(unique, counts)) # CONVERT IT INTO DICTIONARY {label: counts}\n",
    "out = pd.DataFrame(sorted(x.items(), key=lambda kv: kv[0])) #CREATE DATAFRAME FROM DICTIONARY (x)\n",
    "out.drop(0, inplace = True, axis = 1) # DROP EXEEDS(INDEX) COLUMNS\n",
    "out = out.apply(lambda x: 100 * x/sum(x)) # CONVERT COUNT TO PERCENTAGE OF EACH UNIQUE LABEL COUNT เช่น 0:49.89 1:50.10 (%)\n",
    "\n",
    "total_labels = [y for x in labels for y in x] # LIST TO CONTAIN ALL LABELS OF DATA[0,0,0,1,0,0,1]\n",
    "unique, counts = np.unique(total_labels, return_counts=True)\n",
    "y=dict(zip(unique, counts)) #CONVERT INTO DICT {label: counts} (FOR ENTIRE DATASET NO SPLIT)\n",
    "out2 = pd.DataFrame(sorted(y.items(), key=lambda kv: kv[0]))\n",
    "out2.drop(0, inplace = True, axis = 1)\n",
    "out2 = out2.apply(lambda x: 100 * x/sum(x))\n",
    "\n",
    "print(out)\n",
    "print(out2)\n",
    "display(out2.join(out, lsuffix='VALIDATION_SET', rsuffix='ENTIRE_SET')[:5])\n",
    "np.allclose(out.iloc[:,0].values, out2.iloc[:,0].values,  atol=0.01) \n",
    "# Returns True if two arrays are element-wise equal within a tolerance.\n",
    "# ใช้ดูว่าการแบ่งสัดส่วนของ Class target ของ VALIDATION_SET เมื่อเทียบกับ ENTIRE_SET ใกล้เคียงกันไหม"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING PROCESS\n",
    "#### MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "# check format, channel last, (x_train.shape[0], rows, cols, 1)\n",
    "print(keras.backend.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = 32\n",
    "COLUMNS = 40\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "optimizer = keras.optimizers.Adam(learning_rate = LEARNING_RATE)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "MODEL_PATH = \"models\"\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "train_size = train_set.shape[0]\n",
    "validation_size = validation_set.shape[0]\n",
    "steps_per_epoch = train_size//BATCH_SIZE\n",
    "\n",
    "checkpoint_filepath = os.path.join(MODEL_PATH, \n",
    "                                'model.{epoch:02d}-{val_sparse_categorical_accuracy:.2f}-{val_loss:.2f}.h5')\n",
    "\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                    save_weights_only=False,\n",
    "                                                    monitor='val_sparse_categorical_accuracy',\n",
    "                                                    mode='max',\n",
    "                                                    save_best_only=True,\n",
    "                                                    verbose=1)\n",
    "\n",
    "reduce_lr_callback = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                                    patience=3, min_lr=1e-5, vebose=1)\n",
    "earlystop_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                min_delta=1e-3,\n",
    "                                                patience=5,\n",
    "                                                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 40, 32)        320       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 40, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 20, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 20, 64)        18496     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 16, 20, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 10, 64)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 10, 128)        73856     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 8, 10, 128)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 5, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 5, 256)         295168    \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 4, 5, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 2, 2, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               131200    \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 8)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 520,072\n",
      "Trainable params: 520,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = build_model(len(classes_map), (ROWS, COLUMNS, 1))\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[acc_metric])   \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthmlbdshoichi\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>t:\\Shoichi\\Shoichi's Learning\\BrownienLab\\NLP\\SpeechControl\\wandb\\run-20220318_143009-2p2gatoq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/thmlbdshoichi/NLP_SpeechControlTH/runs/2p2gatoq\" target=\"_blank\">feasible-snowflake-31</a></strong> to <a href=\"https://wandb.ai/thmlbdshoichi/NLP_SpeechControlTH\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WANDB LOGING\n",
    "# INITIALIZE WANDB PROJECT AND SPECIFY HYPERPARAMETER DATA\n",
    "run = wandb.init(project='NLP_SpeechControlTH',entity=\"thmlbdshoichi\") #entity = username wandb\n",
    "wandb.config = {\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE\n",
    "}\n",
    "config = wandb.config # CONFIGURE OF EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.0576 - sparse_categorical_accuracy: 0.1649\n",
      "Epoch 1: val_sparse_categorical_accuracy improved from -inf to 0.34375, saving model to models\\model.01-0.34-1.91.h5\n",
      "29/29 [==============================] - 25s 808ms/step - loss: 2.0576 - sparse_categorical_accuracy: 0.1649 - val_loss: 1.9125 - val_sparse_categorical_accuracy: 0.3438 - lr: 0.0010 - _timestamp: 1647588645.0000 - _runtime: 36.0000\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.5005 - sparse_categorical_accuracy: 0.4306\n",
      "Epoch 2: val_sparse_categorical_accuracy improved from 0.34375 to 0.57292, saving model to models\\model.02-0.57-1.09.h5\n",
      "29/29 [==============================] - 21s 742ms/step - loss: 1.5005 - sparse_categorical_accuracy: 0.4306 - val_loss: 1.0889 - val_sparse_categorical_accuracy: 0.5729 - lr: 0.0010 - _timestamp: 1647588666.0000 - _runtime: 57.0000\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.7871 - sparse_categorical_accuracy: 0.7256\n",
      "Epoch 3: val_sparse_categorical_accuracy improved from 0.57292 to 0.84635, saving model to models\\model.03-0.85-0.47.h5\n",
      "29/29 [==============================] - 22s 756ms/step - loss: 0.7871 - sparse_categorical_accuracy: 0.7256 - val_loss: 0.4718 - val_sparse_categorical_accuracy: 0.8464 - lr: 0.0010 - _timestamp: 1647588688.0000 - _runtime: 79.0000\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.4450 - sparse_categorical_accuracy: 0.8612\n",
      "Epoch 4: val_sparse_categorical_accuracy improved from 0.84635 to 0.92448, saving model to models\\model.04-0.92-0.24.h5\n",
      "29/29 [==============================] - 21s 741ms/step - loss: 0.4450 - sparse_categorical_accuracy: 0.8612 - val_loss: 0.2392 - val_sparse_categorical_accuracy: 0.9245 - lr: 0.0010 - _timestamp: 1647588709.0000 - _runtime: 100.0000\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2485 - sparse_categorical_accuracy: 0.9165\n",
      "Epoch 5: val_sparse_categorical_accuracy improved from 0.92448 to 0.93490, saving model to models\\model.05-0.93-0.20.h5\n",
      "29/29 [==============================] - 21s 713ms/step - loss: 0.2485 - sparse_categorical_accuracy: 0.9165 - val_loss: 0.1991 - val_sparse_categorical_accuracy: 0.9349 - lr: 0.0010 - _timestamp: 1647588730.0000 - _runtime: 121.0000\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.1967 - sparse_categorical_accuracy: 0.9447\n",
      "Epoch 6: val_sparse_categorical_accuracy improved from 0.93490 to 0.96354, saving model to models\\model.06-0.96-0.11.h5\n",
      "29/29 [==============================] - 21s 732ms/step - loss: 0.1967 - sparse_categorical_accuracy: 0.9447 - val_loss: 0.1131 - val_sparse_categorical_accuracy: 0.9635 - lr: 0.0010 - _timestamp: 1647588751.0000 - _runtime: 142.0000\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.1496 - sparse_categorical_accuracy: 0.9610\n",
      "Epoch 7: val_sparse_categorical_accuracy improved from 0.96354 to 0.97656, saving model to models\\model.07-0.98-0.08.h5\n",
      "29/29 [==============================] - 21s 725ms/step - loss: 0.1496 - sparse_categorical_accuracy: 0.9610 - val_loss: 0.0783 - val_sparse_categorical_accuracy: 0.9766 - lr: 0.0010 - _timestamp: 1647588772.0000 - _runtime: 163.0000\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.1041 - sparse_categorical_accuracy: 0.9696\n",
      "Epoch 8: val_sparse_categorical_accuracy improved from 0.97656 to 0.98438, saving model to models\\model.08-0.98-0.06.h5\n",
      "29/29 [==============================] - 20s 701ms/step - loss: 0.1041 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.0637 - val_sparse_categorical_accuracy: 0.9844 - lr: 0.0010 - _timestamp: 1647588792.0000 - _runtime: 183.0000\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.1150 - sparse_categorical_accuracy: 0.9642\n",
      "Epoch 9: val_sparse_categorical_accuracy improved from 0.98438 to 0.98958, saving model to models\\model.09-0.99-0.05.h5\n",
      "29/29 [==============================] - 21s 748ms/step - loss: 0.1150 - sparse_categorical_accuracy: 0.9642 - val_loss: 0.0488 - val_sparse_categorical_accuracy: 0.9896 - lr: 0.0010 - _timestamp: 1647588813.0000 - _runtime: 204.0000\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.0830 - sparse_categorical_accuracy: 0.9707\n",
      "Epoch 10: val_sparse_categorical_accuracy did not improve from 0.98958\n",
      "29/29 [==============================] - 22s 755ms/step - loss: 0.0830 - sparse_categorical_accuracy: 0.9707 - val_loss: 0.0374 - val_sparse_categorical_accuracy: 0.9870 - lr: 0.0010 - _timestamp: 1647588835.0000 - _runtime: 226.0000\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.0727 - sparse_categorical_accuracy: 0.9740\n",
      "Epoch 11: val_sparse_categorical_accuracy did not improve from 0.98958\n",
      "29/29 [==============================] - 20s 696ms/step - loss: 0.0727 - sparse_categorical_accuracy: 0.9740 - val_loss: 0.0628 - val_sparse_categorical_accuracy: 0.9844 - lr: 0.0010 - _timestamp: 1647588855.0000 - _runtime: 246.0000\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.1021 - sparse_categorical_accuracy: 0.9761\n",
      "Epoch 12: val_sparse_categorical_accuracy improved from 0.98958 to 0.99479, saving model to models\\model.12-0.99-0.03.h5\n",
      "29/29 [==============================] - 21s 722ms/step - loss: 0.1021 - sparse_categorical_accuracy: 0.9761 - val_loss: 0.0286 - val_sparse_categorical_accuracy: 0.9948 - lr: 0.0010 - _timestamp: 1647588876.0000 - _runtime: 267.0000\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.0391 - sparse_categorical_accuracy: 0.9881\n",
      "Epoch 13: val_sparse_categorical_accuracy did not improve from 0.99479\n",
      "29/29 [==============================] - 20s 700ms/step - loss: 0.0391 - sparse_categorical_accuracy: 0.9881 - val_loss: 0.0322 - val_sparse_categorical_accuracy: 0.9896 - lr: 0.0010 - _timestamp: 1647588896.0000 - _runtime: 287.0000\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.0643 - sparse_categorical_accuracy: 0.9794\n",
      "Epoch 14: val_sparse_categorical_accuracy did not improve from 0.99479\n",
      "29/29 [==============================] - 20s 696ms/step - loss: 0.0643 - sparse_categorical_accuracy: 0.9794 - val_loss: 0.0445 - val_sparse_categorical_accuracy: 0.9844 - lr: 0.0010 - _timestamp: 1647588916.0000 - _runtime: 307.0000\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.0735 - sparse_categorical_accuracy: 0.9783\n",
      "Epoch 15: val_sparse_categorical_accuracy did not improve from 0.99479\n",
      "29/29 [==============================] - 20s 710ms/step - loss: 0.0735 - sparse_categorical_accuracy: 0.9783 - val_loss: 0.0582 - val_sparse_categorical_accuracy: 0.9792 - lr: 0.0010 - _timestamp: 1647588937.0000 - _runtime: 328.0000\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.0495 - sparse_categorical_accuracy: 0.9837\n",
      "Epoch 16: val_sparse_categorical_accuracy improved from 0.99479 to 1.00000, saving model to models\\model.16-1.00-0.01.h5\n",
      "29/29 [==============================] - 21s 717ms/step - loss: 0.0495 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.0096 - val_sparse_categorical_accuracy: 1.0000 - lr: 2.0000e-04 - _timestamp: 1647588957.0000 - _runtime: 348.0000\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.0395 - sparse_categorical_accuracy: 0.9870\n",
      "Epoch 17: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 20s 690ms/step - loss: 0.0395 - sparse_categorical_accuracy: 0.9870 - val_loss: 0.0169 - val_sparse_categorical_accuracy: 0.9948 - lr: 2.0000e-04 - _timestamp: 1647588977.0000 - _runtime: 368.0000\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.0353 - sparse_categorical_accuracy: 0.9892\n",
      "Epoch 18: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 20s 702ms/step - loss: 0.0353 - sparse_categorical_accuracy: 0.9892 - val_loss: 0.0109 - val_sparse_categorical_accuracy: 1.0000 - lr: 2.0000e-04 - _timestamp: 1647588997.0000 - _runtime: 388.0000\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.0453 - sparse_categorical_accuracy: 0.9892\n",
      "Epoch 19: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 20s 696ms/step - loss: 0.0453 - sparse_categorical_accuracy: 0.9892 - val_loss: 0.0326 - val_sparse_categorical_accuracy: 0.9870 - lr: 2.0000e-04 - _timestamp: 1647589017.0000 - _runtime: 408.0000\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.0378 - sparse_categorical_accuracy: 0.9859\n",
      "Epoch 20: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 21s 728ms/step - loss: 0.0378 - sparse_categorical_accuracy: 0.9859 - val_loss: 0.0211 - val_sparse_categorical_accuracy: 0.9896 - lr: 4.0000e-05 - _timestamp: 1647589038.0000 - _runtime: 429.0000\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.0369 - sparse_categorical_accuracy: 0.9870\n",
      "Epoch 21: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 19s 678ms/step - loss: 0.0369 - sparse_categorical_accuracy: 0.9870 - val_loss: 0.0166 - val_sparse_categorical_accuracy: 0.9974 - lr: 4.0000e-05 - _timestamp: 1647589058.0000 - _runtime: 449.0000\n",
      "Epoch 21: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_datagen,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=validation_datagen,\n",
    "                    validation_steps=validation_size//BATCH_SIZE,\n",
    "                    callbacks=[earlystop_callback, reduce_lr_callback, checkpoint_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>loss</td><td>█▆▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████▂▂▂▂▁▁</td></tr><tr><td>sparse_categorical_accuracy</td><td>▁▃▆▇▇████████████████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_sparse_categorical_accuracy</td><td>▁▃▆▇▇████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>15</td></tr><tr><td>best_val_loss</td><td>0.0096</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>loss</td><td>0.03692</td></tr><tr><td>lr</td><td>4e-05</td></tr><tr><td>sparse_categorical_accuracy</td><td>0.98698</td></tr><tr><td>val_loss</td><td>0.01663</td></tr><tr><td>val_sparse_categorical_accuracy</td><td>0.9974</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">feasible-snowflake-31</strong>: <a href=\"https://wandb.ai/thmlbdshoichi/NLP_SpeechControlTH/runs/2p2gatoq\" target=\"_blank\">https://wandb.ai/thmlbdshoichi/NLP_SpeechControlTH/runs/2p2gatoq</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220318_143009-2p2gatoq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WANDB.log\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 40, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 32, 40, 32)        320       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 40, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 20, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 20, 64)        18496     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 16, 20, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 10, 64)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 10, 128)        73856     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 8, 10, 128)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 5, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 5, 256)         295168    \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 4, 5, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 2, 2, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               131200    \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 520,072\n",
      "Trainable params: 520,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CUT LAST LAYER\n",
    "#new_model = model.layers[-1].output\n",
    "model_new = keras.models.Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "model_new.set_weights(model.get_weights())\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 0.01)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "model_new.compile(optimizer=optimizer, loss=loss_fn, metrics=[acc_metric])\n",
    "model_new.summary()\n",
    "model_new.save('model_nsmv3x.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EVALUDATE THE MODEL PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''test_error, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"LOSS ERROR: {test_error*100:.3f}% | ACCURACY: {test_accuracy*100:.3f}%\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keyword_Demo_v2:\n",
    "    def __init__(self, model_path, text_labels, plot=False):\n",
    "        if os.path.exists(model_path):\n",
    "            self.model = keras.models.load_model(model_path)\n",
    "            #self.model.summary()\n",
    "        else:\n",
    "            self.model = None\n",
    "        if os.path.exists(text_labels):\n",
    "            with open(text_labels, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            self.text_labels = [k for k in data.keys()]\n",
    "        else:\n",
    "            self.txt_labels = None\n",
    "        self.plot = plot\n",
    "    \n",
    "    def predict(self, file_path):\n",
    "        # EXTRACT MFCCs\n",
    "        MFCCs = self.preprocess_data_test(file_path)\n",
    "        #MFCCs = MFCCs[np.newaxis, ...]\n",
    "        print(f\"INPUT SHAPE: {MFCCs.shape}\")\n",
    "        # PREDICT -> OUTPUT PROBABILITY\n",
    "        predictions = self.model.predict(MFCCs)\n",
    "        predicted_index = np.argmax(predictions)\n",
    "        predicted_conf = predictions[0][predicted_index]\n",
    "        predicted_label = self.text_labels[predicted_index]\n",
    "        if predicted_conf < 0.7:\n",
    "            predicted_label = 'OTHER_KEYWORDS'\n",
    "            print(f\"Keyword Detected '{predicted_label}' | ORIGINAL: '{self.text_labels[predicted_index]}' NUM:'{predicted_index}' Confidence: {predicted_conf*100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"Keyword Detected '{predicted_label}' | 'NUM:{predicted_index}' Confidence: {predicted_conf*100:.2f}%\")\n",
    "        return predicted_label, predicted_conf\n",
    "    \n",
    "    def preprocess_data_test(self, file_path, n_mfcc=40, n_fft=4096, hop_length=512, NUM_SAMPLES_TO_CONSIDER=16000):\n",
    "        # LOAD AUDIO FILE\n",
    "        signal, sr = librosa.load(file_path, sr=NUM_SAMPLES_TO_CONSIDER)\n",
    "        signal = self.pad_audio(signal, sr)\n",
    "        # EXTRACT MFCCs\n",
    "        MFCCs = librosa.feature.mfcc(y=signal, n_mfcc=n_mfcc,\n",
    "                                    hop_length=hop_length,\n",
    "                                    n_fft=n_fft)\n",
    "        \n",
    "        # PLOT OR NOT\n",
    "        if self.plot:\n",
    "            librosa.display.specshow(MFCCs, sr=sr, hop_length=hop_length)\n",
    "            plt.title(f\"MFCCs Sample {file_path} (BEFORE MFCCs.T)\")\n",
    "            plt.xlabel(\"Time (sec)\")\n",
    "            plt.ylabel(\"MFCC\")\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            \n",
    "        MFCCs = MFCCs.T # MFCCs = np.moveaxis(MFCCs, 1, 0)\n",
    "        scaler = StandardScaler()\n",
    "        MFCCs_scaled = scaler.fit_transform(MFCCs)\n",
    "        MFCCs_scaled = MFCCs_scaled.reshape(MFCCs_scaled.shape[0], MFCCs_scaled.shape[1], 1)\n",
    "        MFCCs_scaled = MFCCs_scaled[np.newaxis, ...]\n",
    "        # MFCCs Input Shape -> (NUM_SAMPLE x NUM_MFCC_COEFFICIENT x 1)\n",
    "        return MFCCs_scaled #IF ERROR BRING MFCCs_scaled back to this line\n",
    "\n",
    "    def pad_audio(self, signal, NUM_SAMPLES_TO_CONSIDER):\n",
    "        if len(signal) >= NUM_SAMPLES_TO_CONSIDER:\n",
    "            return signal[:NUM_SAMPLES_TO_CONSIDER]\n",
    "        else:\n",
    "            return np.pad(signal, pad_width=(NUM_SAMPLES_TO_CONSIDER - len(signal), 0), mode='constant', constant_values=(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models_v2/model-best.h5'\n",
    "model_pred = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = os.path.join(\"Data_Thai\",\"test\")\n",
    "pred_demo = Keyword_Demo_v2(model_path, DATASET_JSON, plot=False)\n",
    "file_test_list = [filenames for _,_,filenames in os.walk(TEST_PATH)][0]\n",
    "for idx, file in enumerate(file_test_list):\n",
    "    print(f\"# {idx+1} {file}---------------------------------------------------\")\n",
    "    AUDIO_DATA_INPUT = os.path.join(\"Data_Thai/test/\",file)\n",
    "    keyword_result = pred_demo.predict(AUDIO_DATA_INPUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_demo2 = Keyword_Demo_v2(model_path, DATASET_JSON, plot=False)\n",
    "for idx, (dirpath, dirnames, filenames) in enumerate(os.walk(DATASET_PATH)):\n",
    "    print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REAL-TIME SPEECH COMMAND RECOGNITION (INFERENCE PROCESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUG - TEST PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vscode_audio import Audio\n",
    "x,sr = librosa.load(train_audio_sample, sr = 16000)\n",
    "x_augmented = random_shift(x)\n",
    "x_augmented = random_speed_up(x_augmented)\n",
    "x_augmented = random_change_pitch(x_augmented)\n",
    "Audio(x_augmented, sr)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "555a84b3913872e2e862f1e806eac0e56227ea6f84c49940b9c2017cb049ce4f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
