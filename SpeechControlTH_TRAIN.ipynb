{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Audio Recognition using Tensorflow\n",
    "### Speech Recognition for Controlling Robot (THAI COMMAND)\n",
    "#### By. Arunwat Moonbung\n",
    "#### SPECIAL THANKS TO \"Leandro Roser\"\n",
    "#### FOR AUDIO-PREPROCESSING e.g. AUDIO-AUGMENTATION TECHNIQUES, DATA LOADING, DATA INTEGRITY OBSERVE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import glob\n",
    "import IPython.display as ipd\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "import librosa, librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,LabelBinarizer\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.python.client import device_lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using Tensorflow 2.8.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-09 13:46:54.615215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 13:46:54.615587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 13:46:54.615862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 13:46:54.616188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 13:46:54.616476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-09 13:46:54.616713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 1279 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0,1'\n",
    "print(f\"Currently using Tensorflow {tf.__version__}\")\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())\n",
    "tf.random.set_seed(221)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASS AND FUNCTION DEFINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join(\"Data_Thai\",\"train\")\n",
    "DATASET_JSON = os.path.join(\"Data_Thai\",\"classmap.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_audio(signal, NUM_SAMPLES_TO_CONSIDER):\n",
    "    if len(signal) >= NUM_SAMPLES_TO_CONSIDER:\n",
    "        return signal[:NUM_SAMPLES_TO_CONSIDER]\n",
    "    else:\n",
    "        return np.pad(signal, pad_width=(NUM_SAMPLES_TO_CONSIDER - len(signal), 0), mode='constant', constant_values=(0, 0))\n",
    "    \n",
    "def pad_audio_sec(self, signal, DURATION, NUM_SAMPLES_TO_CONSIDER):\n",
    "        TOTAL_SAMPLE = DURATION*NUM_SAMPLES_TO_CONSIDER\n",
    "        if len(signal) >= TOTAL_SAMPLE:\n",
    "            return signal[:TOTAL_SAMPLE]\n",
    "        else:\n",
    "            #return np.pad(signal, pad_width=(0, TOTAL_SAMPLE - len(signal)), mode='constant', constant_values=(0, 0)) # PAD หลัง\n",
    "            return np.pad(signal, pad_width=(TOTAL_SAMPLE - len(signal), 0), mode='constant', constant_values=(0, 0)) # PAD หน้า\n",
    "\n",
    "def chop_audio(signal, NUM_SAMPLES_TO_CONSIDER=16000):\n",
    "    while True:\n",
    "        beg = np.random.randint(0, len(signal) - NUM_SAMPLES_TO_CONSIDER)\n",
    "        yield signal[beg: beg + NUM_SAMPLES_TO_CONSIDER]\n",
    "\n",
    "def choose_background_generator(signal, backgrounds, max_alpha=0.7):\n",
    "    if backgrounds is None:\n",
    "        return signal\n",
    "    my_gen = backgrounds[np.random.randint(len(backgrounds))]\n",
    "    background = next(my_gen) * np.random.uniform(0, max_alpha)\n",
    "    augmented_data = signal + background\n",
    "    augmented_data = augmented_data.astype(type(signal[0]))\n",
    "    return augmented_data\n",
    "\n",
    "def random_shift(signal, NUM_SAMPLES_TO_CONSIDER=16000, shift_max=0.2):\n",
    "    shift = np.random.randint(NUM_SAMPLES_TO_CONSIDER * shift_max)\n",
    "    out = np.roll(signal, shift)\n",
    "    # Time shift\n",
    "    if shift > 0:\n",
    "        out[:shift] = 0\n",
    "    else:\n",
    "        out[shift:] = 0\n",
    "    return out\n",
    "\n",
    "def random_change_pitch(signal, NUM_SAMPLES_TO_CONSIDER=16000):\n",
    "    pitch_factor = np.random.randint(1, 4)\n",
    "    out = librosa.effects.pitch_shift(y=signal, sr=NUM_SAMPLES_TO_CONSIDER, n_steps=pitch_factor)\n",
    "    return out\n",
    "\n",
    "def random_speed_up(signal):\n",
    "    where = [\"start\", \"end\"][np.random.randint(0, 1)]\n",
    "    speed_factor = np.random.uniform(0, 0.5)\n",
    "    up = librosa.effects.time_stretch(y=signal, rate=1 + speed_factor)\n",
    "    up_len = up.shape[0]\n",
    "    if where == \"end\":\n",
    "        up = np.concatenate((up, np.zeros((signal.shape[0] - up_len,))))\n",
    "    else:\n",
    "        up = np.concatenate((np.zeros((signal.shape[0] - up_len,)), up))\n",
    "    return up\n",
    "\n",
    "def get_image_list(train_audio_path):\n",
    "    classes = os.listdir(train_audio_path)\n",
    "    classes = [thisclass for thisclass in classes if thisclass != '_background_noise_']\n",
    "    index = [i for i,j in enumerate(classes)]\n",
    "    outlist = []\n",
    "    labels = []\n",
    "    text_labels = dict(zip(classes, index))\n",
    "    for thisindex, thisclass in zip(index, classes):\n",
    "        filelist = [f for f in os.listdir(os.path.join(train_audio_path, thisclass)) if f.endswith('.wav')]\n",
    "        filelist = [os.path.join(train_audio_path, thisclass, x) for x in filelist]\n",
    "        outlist.append(filelist)\n",
    "        labels.append(np.full(len(filelist), fill_value=thisindex))\n",
    "    try:\n",
    "        with open(DATASET_JSON, \"w\") as f:\n",
    "            json.dump(text_labels, f, indent=4)\n",
    "        print(f\"#: SAVING CLASS LABEL MAPPING.. AT {train_audio_path}\")\n",
    "    except:\n",
    "        print(\"!: ERROR WHILE SAVING .json file\")\n",
    "        \n",
    "    return outlist, labels, text_labels\n",
    "\n",
    "def split_train_test_stratified_shuffle(images_list, labels, train_size=0.7):\n",
    "    classes_size = [len(x) for x in images_list]\n",
    "    classes_vector = [np.arange(x) for x in classes_size]\n",
    "    total = np.sum(classes_size)\n",
    "    total_train = [int(train_size * total * x) for x in classes_size / total]\n",
    "    train_index = [np.random.choice(x, y, replace=False) for x,y in zip(classes_size, total_train)]\n",
    "    validation_index = [np.setdiff1d(i,j) for i,j in zip(classes_vector,train_index)]\n",
    "    train_set = [np.array(x)[idx] for x,idx in zip(images_list, train_index)]\n",
    "    validation_set = [np.array(x)[idx] for x,idx in zip(images_list, validation_index)]\n",
    "    train_labels = [np.array(x)[idx] for x,idx in zip(labels,train_index)]\n",
    "    validation_labels = [np.array(x)[idx] for x,idx in zip(labels, validation_index)]\n",
    "    # ----------------------------------------------------------------------------------------- \n",
    "    train_set = np.array([element for array in train_set for element in array])\n",
    "    validation_set = np.array([element for array in validation_set for element in array])\n",
    "    train_labels = np.array([element for array in train_labels for element in array])\n",
    "    validation_labels = np.array([element for array in validation_labels for element in array])\n",
    "    # -----------------------------------------------------------------------------------------\n",
    "    train_shuffle = np.random.permutation(len(train_set))\n",
    "    validation_shuffle = np.random.permutation(len(validation_set))\n",
    "    train_set = train_set[train_shuffle]\n",
    "    validation_set = validation_set[validation_shuffle]\n",
    "    train_labels = train_labels[train_shuffle]\n",
    "    validation_labels = validation_labels[validation_shuffle]\n",
    "    return train_set, train_labels, validation_set, validation_labels\n",
    "\n",
    "def preprocess_data(file_path, background_generator, n_mfcc=40, hop_length=512, n_fft=4096, NUM_SAMPLES_TO_CONSIDER=16000, threshold=0.7):\n",
    "    # Downsample to NUM_SAMPLES_TO_CONSIDER Hz\n",
    "    signal, sr = librosa.load(file_path, sr=NUM_SAMPLES_TO_CONSIDER)\n",
    "    signal = pad_audio(signal, sr)\n",
    "    if np.random.uniform(0, 1) > threshold:\n",
    "        # ADD NOISE TO 30% OF DATA\n",
    "        signal = choose_background_generator(signal, background_generator)\n",
    "    if np.random.uniform(0, 1) > threshold:\n",
    "        signal = random_shift(signal)\n",
    "    if np.random.uniform(0, 1) > threshold:\n",
    "        signal = random_change_pitch(signal)\n",
    "    if np.random.uniform(0, 1) > threshold:\n",
    "        signal = random_speed_up(signal)\n",
    "    MFCCs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=n_mfcc) # Transpose for sklearn\n",
    "    MFCCs = np.moveaxis(MFCCs, 1, 0)\n",
    "    #scaler = MinMaxScaler() # OPTIONAL FOR Scaling\n",
    "    scaler = StandardScaler() \n",
    "    MFCCs_scaled = scaler.fit_transform(MFCCs)\n",
    "    # MFCCs Input Shape -> (NUM_SAMPLE x NUM_MFCC_COEFFICIENT x 1)\n",
    "    return MFCCs_scaled.reshape(MFCCs_scaled.shape[0], MFCCs_scaled.shape[1], 1)\n",
    "\n",
    "class data_generator(keras.utils.Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size, background_generator):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.background_generator = background_generator\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx_from = idx * self.batch_size\n",
    "        idx_to = (idx + 1) * self.batch_size\n",
    "        batch_x = self.x[idx_from:idx_to]\n",
    "        batch_y = self.y[idx_from:idx_to]\n",
    "        x = [preprocess_data(elem, self.background_generator) for elem in batch_x]\n",
    "        y = batch_y\n",
    "        return np.array(x).astype(np.float32), np.array(y).astype(np.float32)\n",
    "        #return np.array(x), np.array(y)\n",
    "    \n",
    "def build_model(num_classes, input_shape):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.Input(shape=input_shape))\n",
    "    model.add(keras.layers.Conv2D(filters=32, kernel_size=(3,3), \n",
    "                                padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Conv2D(filters=64, kernel_size=(3,3),\n",
    "                                padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Conv2D(filters=128, kernel_size=(3,3),\n",
    "                                padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Conv2D(filters=256, kernel_size=(3,3),\n",
    "                                padding=\"same\", activation=\"relu\",\n",
    "                                kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(keras.layers.Dropout(0.25))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(128, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def multiclass_roc(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "    y_pred = lb.transform(y_pred)\n",
    "    all_labels = np.unique(y_test)\n",
    "    \n",
    "    for (idx, c_label) in enumerate(all_labels):\n",
    "        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])\n",
    "        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)' % (c_label, auc(fpr, tpr)))\n",
    "    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n",
    "    return roc_auc_score(y_test, y_pred, average=average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train_audio_sample = os.path.join(\"Data_Thai\",\"train\",\"backward\",\"b1.wav\")\n",
    "x,sr = librosa.load(train_audio_sample, sr = 16000)\n",
    "x = pad_audio(x, sr)\n",
    "choose_background_generator(x, background_generator)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING DATASET / VERIFY LOADING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA WITH BACKGROUNDS\n",
    "wavfiles = glob.glob(os.path.join(DATASET_PATH, \"_background_noise_/*wav\"))\n",
    "wavfiles = [librosa.load(elem, sr = 16000)[0] for elem in wavfiles]\n",
    "# wavfile คือ array ของไฟล์เสียง\n",
    "background_generator = [chop_audio(x) for x in wavfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#: SAVING CLASS LABEL MAPPING.. AT Data_Thai\\train\n"
     ]
    }
   ],
   "source": [
    "# TRAINING DATA PATHS, \n",
    "# SPLIT TRAIN-TEST VIA STRATIFIED SAMPLING, \n",
    "# CALL A DATA GENERATOR FOR KERAS\n",
    "# LOAD TRAIN\n",
    "images_list, labels, classes_map = get_image_list(DATASET_PATH)\n",
    "train_set, train_labels, validation_set, validation_labels = split_train_test_stratified_shuffle(images_list, labels)\n",
    "train_datagen = data_generator(x_set=train_set, y_set=train_labels, batch_size=32, background_generator=background_generator)\n",
    "validation_datagen = data_generator(x_set=validation_set, y_set=validation_labels, batch_size=32, background_generator=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(train_datagen[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''display(images_list)\n",
    "display(labels)\n",
    "display(classes_map)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK IF TRAINING DATASET ALSO CONTAIN IN VALIDATION DATASET? // ควรเป็น False เพราะต้องไม่มีตัวไหนที่ซ้ำกัน\n",
    "inv_map =  {v: k for k, v in classes_map.items()}\n",
    "any_present=[i in validation_set for i in train_set]\n",
    "np.any(any_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Data_Thai\\\\train\\\\grab\\\\g604.wav',\n",
       "        'Data_Thai\\\\train\\\\stop\\\\s802.wav',\n",
       "        'Data_Thai\\\\train\\\\turnleft\\\\l508.wav',\n",
       "        'Data_Thai\\\\train\\\\grab\\\\g209.wav',\n",
       "        'Data_Thai\\\\train\\\\turnleft\\\\l610.wav',\n",
       "        'Data_Thai\\\\train\\\\backward\\\\b213.wav',\n",
       "        'Data_Thai\\\\train\\\\backward\\\\b18.wav',\n",
       "        'Data_Thai\\\\train\\\\release\\\\re602.wav',\n",
       "        'Data_Thai\\\\train\\\\release\\\\re118.wav',\n",
       "        'Data_Thai\\\\train\\\\forward\\\\f802.wav'], dtype='<U34'),\n",
       " ['grab',\n",
       "  'stop',\n",
       "  'turnleft',\n",
       "  'grab',\n",
       "  'turnleft',\n",
       "  'backward',\n",
       "  'backward',\n",
       "  'release',\n",
       "  'release',\n",
       "  'forward'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK MATCHING FILE .WAV FROM (train_set) and LABELS FROM (train_labels) \n",
    "# โฟลเดอร์ไฟล์ ต้องตรงกับ Label ข้างล่างตามลำดับ ไม่งั้นแสดงว่า Data-pre ผิดพลาด\n",
    "test1 = np.random.randint(10, 100, 10)\n",
    "train_set[test1],[inv_map[int(i)] for i in train_labels[test1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           1\n",
      "0  12.590799\n",
      "1  12.590799\n",
      "2  11.864407\n",
      "3  12.590799\n",
      "4  12.106538\n",
      "5  12.348668\n",
      "6  12.832930\n",
      "7  13.075061\n",
      "           1\n",
      "0  12.509144\n",
      "1  12.655450\n",
      "2  11.923921\n",
      "3  12.582297\n",
      "4  12.143380\n",
      "5  12.435991\n",
      "6  12.728603\n",
      "7  13.021214\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1VALIDATION_SET</th>\n",
       "      <th>1ENTIRE_SET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.509144</td>\n",
       "      <td>12.590799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.655450</td>\n",
       "      <td>12.590799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.923921</td>\n",
       "      <td>11.864407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.582297</td>\n",
       "      <td>12.590799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.143380</td>\n",
       "      <td>12.106538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1VALIDATION_SET  1ENTIRE_SET\n",
       "0        12.509144    12.590799\n",
       "1        12.655450    12.590799\n",
       "2        11.923921    11.864407\n",
       "3        12.582297    12.590799\n",
       "4        12.143380    12.106538"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STRATIFIED SAMPLING WORKS\n",
    "# CHECK UNIQUE VALUE IN NUM_CLASSES (unique) [0, 1, ...]\n",
    "# COUNT NUM_SAMPLE IN EACH UNIQUE VALUE (counts) [500, 500, ...]\n",
    "unique, counts = np.unique(validation_labels, return_counts=True)\n",
    "x = dict(zip(unique, counts)) # CONVERT IT INTO DICTIONARY {label: counts}\n",
    "out = pd.DataFrame(sorted(x.items(), key=lambda kv: kv[0])) #CREATE DATAFRAME FROM DICTIONARY (x)\n",
    "out.drop(0, inplace = True, axis = 1) # DROP EXEEDS(INDEX) COLUMNS\n",
    "out = out.apply(lambda x: 100 * x/sum(x)) # CONVERT COUNT TO PERCENTAGE OF EACH UNIQUE LABEL COUNT เช่น 0:49.89 1:50.10 (%)\n",
    "\n",
    "total_labels = [y for x in labels for y in x] # LIST TO CONTAIN ALL LABELS OF DATA[0,0,0,1,0,0,1]\n",
    "unique, counts = np.unique(total_labels, return_counts=True)\n",
    "y=dict(zip(unique, counts)) #CONVERT INTO DICT {label: counts} (FOR ENTIRE DATASET NO SPLIT)\n",
    "out2 = pd.DataFrame(sorted(y.items(), key=lambda kv: kv[0]))\n",
    "out2.drop(0, inplace = True, axis = 1)\n",
    "out2 = out2.apply(lambda x: 100 * x/sum(x))\n",
    "\n",
    "print(out)\n",
    "print(out2)\n",
    "display(out2.join(out, lsuffix='VALIDATION_SET', rsuffix='ENTIRE_SET')[:5])\n",
    "np.allclose(out.iloc[:,0].values, out2.iloc[:,0].values,  atol=0.01) \n",
    "# Returns True if two arrays are element-wise equal within a tolerance.\n",
    "# ใช้ดูว่าการแบ่งสัดส่วนของ Class target ของ VALIDATION_SET เมื่อเทียบกับ ENTIRE_SET ใกล้เคียงกันไหม"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING PROCESS\n",
    "#### MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "# check format, channel last, (x_train.shape[0], rows, cols, 1)\n",
    "print(keras.backend.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = 32\n",
    "COLUMNS = 40\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "optimizer = keras.optimizers.Adam(learning_rate = LEARNING_RATE)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "MODEL_PATH = \"models\"\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "train_size = train_set.shape[0]\n",
    "validation_size = validation_set.shape[0]\n",
    "steps_per_epoch = train_size//BATCH_SIZE\n",
    "\n",
    "checkpoint_filepath = os.path.join(MODEL_PATH, \n",
    "                                'model.{epoch:02d}-{val_sparse_categorical_accuracy:.2f}-{val_loss:.2f}.h5')\n",
    "\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                                    save_weights_only=False,\n",
    "                                                    monitor='val_sparse_categorical_accuracy',\n",
    "                                                    mode='max',\n",
    "                                                    save_best_only=True,\n",
    "                                                    verbose=1)\n",
    "\n",
    "reduce_lr_callback = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                                    patience=3, min_lr=1e-5, vebose=1)\n",
    "earlystop_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                min_delta=1e-3,\n",
    "                                                patience=5,\n",
    "                                                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 40, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 20, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 20, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 10, 64)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 10, 128)        73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 5, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 5, 256)         295168    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 2, 2, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 1032      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 520,072\n",
      "Trainable params: 520,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "#model = build_model(8, (ROWS, COLUMNS, 1))\n",
    "model = build_model(len(classes_map), (ROWS, COLUMNS, 1))\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[acc_metric])   \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthmlbdshoichi\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/thmlbdshoichi/NLP_SpeechControlTH/runs/2gerzud5\" target=\"_blank\">colorful-waterfall-24</a></strong> to <a href=\"https://wandb.ai/thmlbdshoichi/NLP_SpeechControlTH\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WANDB LOGING\n",
    "# INITIALIZE WANDB PROJECT AND SPECIFY HYPERPARAMETER DATA\n",
    "run = wandb.init(project='NLP_SpeechControlTH',entity=\"thmlbdshoichi\") #entity = username wandb\n",
    "wandb.config = {\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE\n",
    "}\n",
    "config = wandb.config # CONFIGURE OF EXPERIMENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.3482 - sparse_categorical_accuracy: 0.1410\n",
      "Epoch 1: val_sparse_categorical_accuracy improved from -inf to 0.21354, saving model to models\\model.01-0.21-2.29.h5\n",
      "29/29 [==============================] - 24s 756ms/step - loss: 2.3482 - sparse_categorical_accuracy: 0.1410 - val_loss: 2.2852 - val_sparse_categorical_accuracy: 0.2135 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 2.1892 - sparse_categorical_accuracy: 0.2343\n",
      "Epoch 2: val_sparse_categorical_accuracy improved from 0.21354 to 0.44010, saving model to models\\model.02-0.44-1.98.h5\n",
      "29/29 [==============================] - 21s 721ms/step - loss: 2.1892 - sparse_categorical_accuracy: 0.2343 - val_loss: 1.9810 - val_sparse_categorical_accuracy: 0.4401 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.7077 - sparse_categorical_accuracy: 0.4046\n",
      "Epoch 3: val_sparse_categorical_accuracy improved from 0.44010 to 0.71875, saving model to models\\model.03-0.72-1.14.h5\n",
      "29/29 [==============================] - 20s 694ms/step - loss: 1.7077 - sparse_categorical_accuracy: 0.4046 - val_loss: 1.1383 - val_sparse_categorical_accuracy: 0.7188 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 1.2202 - sparse_categorical_accuracy: 0.6074\n",
      "Epoch 4: val_sparse_categorical_accuracy improved from 0.71875 to 0.76302, saving model to models\\model.04-0.76-0.88.h5\n",
      "29/29 [==============================] - 20s 692ms/step - loss: 1.2202 - sparse_categorical_accuracy: 0.6074 - val_loss: 0.8788 - val_sparse_categorical_accuracy: 0.7630 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.8926 - sparse_categorical_accuracy: 0.7549\n",
      "Epoch 5: val_sparse_categorical_accuracy improved from 0.76302 to 0.89583, saving model to models\\model.05-0.90-0.53.h5\n",
      "29/29 [==============================] - 20s 681ms/step - loss: 0.8926 - sparse_categorical_accuracy: 0.7549 - val_loss: 0.5318 - val_sparse_categorical_accuracy: 0.8958 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.6301 - sparse_categorical_accuracy: 0.8579\n",
      "Epoch 6: val_sparse_categorical_accuracy improved from 0.89583 to 0.92969, saving model to models\\model.06-0.93-0.43.h5\n",
      "29/29 [==============================] - 20s 710ms/step - loss: 0.6301 - sparse_categorical_accuracy: 0.8579 - val_loss: 0.4317 - val_sparse_categorical_accuracy: 0.9297 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.5116 - sparse_categorical_accuracy: 0.8915\n",
      "Epoch 7: val_sparse_categorical_accuracy improved from 0.92969 to 0.94792, saving model to models\\model.07-0.95-0.38.h5\n",
      "29/29 [==============================] - 21s 732ms/step - loss: 0.5116 - sparse_categorical_accuracy: 0.8915 - val_loss: 0.3773 - val_sparse_categorical_accuracy: 0.9479 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.4327 - sparse_categorical_accuracy: 0.9328\n",
      "Epoch 8: val_sparse_categorical_accuracy did not improve from 0.94792\n",
      "29/29 [==============================] - 21s 719ms/step - loss: 0.4327 - sparse_categorical_accuracy: 0.9328 - val_loss: 0.3620 - val_sparse_categorical_accuracy: 0.9323 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.3576 - sparse_categorical_accuracy: 0.9414\n",
      "Epoch 9: val_sparse_categorical_accuracy improved from 0.94792 to 0.96354, saving model to models\\model.09-0.96-0.32.h5\n",
      "29/29 [==============================] - 20s 694ms/step - loss: 0.3576 - sparse_categorical_accuracy: 0.9414 - val_loss: 0.3229 - val_sparse_categorical_accuracy: 0.9635 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.3377 - sparse_categorical_accuracy: 0.9469\n",
      "Epoch 10: val_sparse_categorical_accuracy improved from 0.96354 to 0.98698, saving model to models\\model.10-0.99-0.23.h5\n",
      "29/29 [==============================] - 21s 712ms/step - loss: 0.3377 - sparse_categorical_accuracy: 0.9469 - val_loss: 0.2299 - val_sparse_categorical_accuracy: 0.9870 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2987 - sparse_categorical_accuracy: 0.9610\n",
      "Epoch 11: val_sparse_categorical_accuracy did not improve from 0.98698\n",
      "29/29 [==============================] - 20s 694ms/step - loss: 0.2987 - sparse_categorical_accuracy: 0.9610 - val_loss: 0.2428 - val_sparse_categorical_accuracy: 0.9766 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2953 - sparse_categorical_accuracy: 0.9620\n",
      "Epoch 12: val_sparse_categorical_accuracy did not improve from 0.98698\n",
      "29/29 [==============================] - 20s 694ms/step - loss: 0.2953 - sparse_categorical_accuracy: 0.9620 - val_loss: 0.2182 - val_sparse_categorical_accuracy: 0.9844 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2555 - sparse_categorical_accuracy: 0.9707\n",
      "Epoch 13: val_sparse_categorical_accuracy improved from 0.98698 to 0.98958, saving model to models\\model.13-0.99-0.20.h5\n",
      "29/29 [==============================] - 20s 697ms/step - loss: 0.2555 - sparse_categorical_accuracy: 0.9707 - val_loss: 0.2041 - val_sparse_categorical_accuracy: 0.9896 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2717 - sparse_categorical_accuracy: 0.9620\n",
      "Epoch 14: val_sparse_categorical_accuracy did not improve from 0.98958\n",
      "29/29 [==============================] - 24s 833ms/step - loss: 0.2717 - sparse_categorical_accuracy: 0.9620 - val_loss: 0.2257 - val_sparse_categorical_accuracy: 0.9818 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2729 - sparse_categorical_accuracy: 0.9707\n",
      "Epoch 15: val_sparse_categorical_accuracy improved from 0.98958 to 0.99219, saving model to models\\model.15-0.99-0.20.h5\n",
      "29/29 [==============================] - 25s 867ms/step - loss: 0.2729 - sparse_categorical_accuracy: 0.9707 - val_loss: 0.1965 - val_sparse_categorical_accuracy: 0.9922 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2827 - sparse_categorical_accuracy: 0.9664\n",
      "Epoch 16: val_sparse_categorical_accuracy improved from 0.99219 to 1.00000, saving model to models\\model.16-1.00-0.18.h5\n",
      "29/29 [==============================] - 23s 797ms/step - loss: 0.2827 - sparse_categorical_accuracy: 0.9664 - val_loss: 0.1830 - val_sparse_categorical_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2691 - sparse_categorical_accuracy: 0.9610\n",
      "Epoch 17: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 22s 769ms/step - loss: 0.2691 - sparse_categorical_accuracy: 0.9610 - val_loss: 0.1932 - val_sparse_categorical_accuracy: 0.9922 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2620 - sparse_categorical_accuracy: 0.9707\n",
      "Epoch 18: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 23s 804ms/step - loss: 0.2620 - sparse_categorical_accuracy: 0.9707 - val_loss: 0.2041 - val_sparse_categorical_accuracy: 0.9896 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2389 - sparse_categorical_accuracy: 0.9707\n",
      "Epoch 19: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 22s 761ms/step - loss: 0.2389 - sparse_categorical_accuracy: 0.9707 - val_loss: 0.2124 - val_sparse_categorical_accuracy: 0.9818 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2404 - sparse_categorical_accuracy: 0.9751\n",
      "Epoch 20: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 22s 764ms/step - loss: 0.2404 - sparse_categorical_accuracy: 0.9751 - val_loss: 0.1724 - val_sparse_categorical_accuracy: 0.9948 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2403 - sparse_categorical_accuracy: 0.9696\n",
      "Epoch 21: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 23s 786ms/step - loss: 0.2403 - sparse_categorical_accuracy: 0.9696 - val_loss: 0.1687 - val_sparse_categorical_accuracy: 0.9974 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2100 - sparse_categorical_accuracy: 0.9837\n",
      "Epoch 22: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 23s 787ms/step - loss: 0.2100 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.1617 - val_sparse_categorical_accuracy: 0.9974 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2041 - sparse_categorical_accuracy: 0.9870\n",
      "Epoch 23: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 23s 805ms/step - loss: 0.2041 - sparse_categorical_accuracy: 0.9870 - val_loss: 0.1679 - val_sparse_categorical_accuracy: 0.9974 - lr: 2.0000e-04\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2194 - sparse_categorical_accuracy: 0.9837\n",
      "Epoch 24: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 23s 811ms/step - loss: 0.2194 - sparse_categorical_accuracy: 0.9837 - val_loss: 0.1805 - val_sparse_categorical_accuracy: 0.9922 - lr: 2.0000e-04\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.1996 - sparse_categorical_accuracy: 0.9870\n",
      "Epoch 25: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 23s 788ms/step - loss: 0.1996 - sparse_categorical_accuracy: 0.9870 - val_loss: 0.1585 - val_sparse_categorical_accuracy: 0.9974 - lr: 2.0000e-04\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2106 - sparse_categorical_accuracy: 0.9805\n",
      "Epoch 26: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 22s 746ms/step - loss: 0.2106 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.1525 - val_sparse_categorical_accuracy: 1.0000 - lr: 2.0000e-04\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.2063 - sparse_categorical_accuracy: 0.9783\n",
      "Epoch 27: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 22s 774ms/step - loss: 0.2063 - sparse_categorical_accuracy: 0.9783 - val_loss: 0.1590 - val_sparse_categorical_accuracy: 0.9948 - lr: 2.0000e-04\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.1978 - sparse_categorical_accuracy: 0.9848\n",
      "Epoch 28: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 23s 788ms/step - loss: 0.1978 - sparse_categorical_accuracy: 0.9848 - val_loss: 0.1591 - val_sparse_categorical_accuracy: 0.9974 - lr: 2.0000e-04\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.1921 - sparse_categorical_accuracy: 0.9881\n",
      "Epoch 29: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 21s 738ms/step - loss: 0.1921 - sparse_categorical_accuracy: 0.9881 - val_loss: 0.1803 - val_sparse_categorical_accuracy: 0.9870 - lr: 2.0000e-04\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.1772 - sparse_categorical_accuracy: 0.9924\n",
      "Epoch 30: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 22s 754ms/step - loss: 0.1772 - sparse_categorical_accuracy: 0.9924 - val_loss: 0.1547 - val_sparse_categorical_accuracy: 1.0000 - lr: 4.0000e-05\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - ETA: 0s - loss: 0.1977 - sparse_categorical_accuracy: 0.9870\n",
      "Epoch 31: val_sparse_categorical_accuracy did not improve from 1.00000\n",
      "29/29 [==============================] - 23s 809ms/step - loss: 0.1977 - sparse_categorical_accuracy: 0.9870 - val_loss: 0.1544 - val_sparse_categorical_accuracy: 0.9948 - lr: 4.0000e-05\n",
      "Epoch 31: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_datagen,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=validation_datagen,\n",
    "                    validation_steps=validation_size//BATCH_SIZE,\n",
    "                    callbacks=[earlystop_callback, reduce_lr_callback, checkpoint_callback, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 27432... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>loss</td><td>█▇▆▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████████████████▂▂▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>sparse_categorical_accuracy</td><td>▁▂▃▅▆▇▇████████████████████████</td></tr><tr><td>val_loss</td><td>█▇▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_sparse_categorical_accuracy</td><td>▁▃▅▆▇▇█▇███████████████████████</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>25</td></tr><tr><td>best_val_loss</td><td>0.15245</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>loss</td><td>0.19774</td></tr><tr><td>lr</td><td>4e-05</td></tr><tr><td>sparse_categorical_accuracy</td><td>0.98698</td></tr><tr><td>val_loss</td><td>0.15439</td></tr><tr><td>val_sparse_categorical_accuracy</td><td>0.99479</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">colorful-waterfall-24</strong>: <a href=\"https://wandb.ai/thmlbdshoichi/NLP_SpeechControlTH/runs/2gerzud5\" target=\"_blank\">https://wandb.ai/thmlbdshoichi/NLP_SpeechControlTH/runs/2gerzud5</a><br/>\n",
       "Find logs at: <code>.\\wandb\\run-20220302_090703-2gerzud5\\logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WANDB.log\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EVALUDATE THE MODEL PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''test_error, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"LOSS ERROR: {test_error*100:.3f}% | ACCURACY: {test_accuracy*100:.3f}%\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keyword_Demo_v2:\n",
    "    def __init__(self, model_path, text_labels, plot=False):\n",
    "        if os.path.exists(model_path):\n",
    "            self.model = keras.models.load_model(model_path)\n",
    "            #self.model.summary()\n",
    "        else:\n",
    "            self.model = None\n",
    "        if os.path.exists(text_labels):\n",
    "            with open(text_labels, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "            self.text_labels = [k for k in data.keys()]\n",
    "        else:\n",
    "            self.txt_labels = None\n",
    "        self.plot = plot\n",
    "    \n",
    "    def predict(self, file_path):\n",
    "        # EXTRACT MFCCs\n",
    "        MFCCs = self.preprocess_data_test(file_path)\n",
    "        #MFCCs = MFCCs[np.newaxis, ...]\n",
    "        print(f\"INPUT SHAPE: {MFCCs.shape}\")\n",
    "        # PREDICT -> OUTPUT PROBABILITY\n",
    "        predictions = self.model.predict(MFCCs)\n",
    "        predicted_index = np.argmax(predictions)\n",
    "        predicted_conf = predictions[0][predicted_index]\n",
    "        predicted_label = self.text_labels[predicted_index]\n",
    "        if predicted_conf < 0.7:\n",
    "            predicted_label = 'OTHER_KEYWORDS'\n",
    "            print(f\"Keyword Detected '{predicted_label}' | ORIGINAL: '{self.text_labels[predicted_index]}' NUM:'{predicted_index}' Confidence: {predicted_conf*100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"Keyword Detected '{predicted_label}' | 'NUM:{predicted_index}' Confidence: {predicted_conf*100:.2f}%\")\n",
    "        return predicted_label, predicted_conf\n",
    "    \n",
    "    def preprocess_data_test(self, file_path, n_mfcc=40, n_fft=4096, hop_length=512, NUM_SAMPLES_TO_CONSIDER=16000):\n",
    "        # LOAD AUDIO FILE\n",
    "        signal, sr = librosa.load(file_path, sr=NUM_SAMPLES_TO_CONSIDER)\n",
    "        signal = self.pad_audio(signal, sr)\n",
    "        # EXTRACT MFCCs\n",
    "        MFCCs = librosa.feature.mfcc(y=signal, n_mfcc=n_mfcc,\n",
    "                                    hop_length=hop_length,\n",
    "                                    n_fft=n_fft)\n",
    "        \n",
    "        # PLOT OR NOT\n",
    "        if self.plot:\n",
    "            librosa.display.specshow(MFCCs, sr=sr, hop_length=hop_length)\n",
    "            plt.title(f\"MFCCs Sample {file_path} (BEFORE MFCCs.T)\")\n",
    "            plt.xlabel(\"Time (sec)\")\n",
    "            plt.ylabel(\"MFCC\")\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            \n",
    "        MFCCs = MFCCs.T # MFCCs = np.moveaxis(MFCCs, 1, 0)\n",
    "        scaler = StandardScaler()\n",
    "        MFCCs_scaled = scaler.fit_transform(MFCCs)\n",
    "        MFCCs_scaled = MFCCs_scaled.reshape(MFCCs_scaled.shape[0], MFCCs_scaled.shape[1], 1)\n",
    "        MFCCs_scaled = MFCCs_scaled[np.newaxis, ...]\n",
    "        # MFCCs Input Shape -> (NUM_SAMPLE x NUM_MFCC_COEFFICIENT x 1)\n",
    "        return MFCCs_scaled #IF ERROR BRING MFCCs_scaled back to this line\n",
    "\n",
    "    def pad_audio(self, signal, NUM_SAMPLES_TO_CONSIDER):\n",
    "        if len(signal) >= NUM_SAMPLES_TO_CONSIDER:\n",
    "            return signal[:NUM_SAMPLES_TO_CONSIDER]\n",
    "        else:\n",
    "            return np.pad(signal, pad_width=(NUM_SAMPLES_TO_CONSIDER - len(signal), 0), mode='constant', constant_values=(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/model-best.h5'\n",
    "model_pred = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1 b30.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'release' NUM:'3' Confidence: 64.62%\n",
      "# 2 b31.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'backward' NUM:'0' Confidence: 30.74%\n",
      "# 3 b32.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'search' | 'NUM:4' Confidence: 96.90%\n",
      "# 4 b33.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'backward' NUM:'0' Confidence: 48.81%\n",
      "# 5 b34.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'search' | 'NUM:4' Confidence: 84.39%\n",
      "# 6 b35.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'release' NUM:'3' Confidence: 63.39%\n",
      "# 7 b36.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'release' NUM:'3' Confidence: 52.99%\n",
      "# 8 b37.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 88.07%\n",
      "# 9 b38.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'backward' NUM:'0' Confidence: 59.93%\n",
      "# 10 b39.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'release' NUM:'3' Confidence: 62.00%\n",
      "# 11 b40.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'release' NUM:'3' Confidence: 49.60%\n",
      "# 12 b71.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'backward' NUM:'0' Confidence: 63.51%\n",
      "# 13 b72.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'grab' | 'NUM:2' Confidence: 94.77%\n",
      "# 14 b73.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'backward' NUM:'0' Confidence: 38.97%\n",
      "# 15 f30.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'turnright' NUM:'7' Confidence: 62.25%\n",
      "# 16 f31.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'forward' | 'NUM:1' Confidence: 82.68%\n",
      "# 17 f32.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'forward' NUM:'1' Confidence: 66.57%\n",
      "# 18 f33.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'forward' NUM:'1' Confidence: 58.10%\n",
      "# 19 f34.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'search' NUM:'4' Confidence: 35.12%\n",
      "# 20 f35.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'forward' NUM:'1' Confidence: 33.23%\n",
      "# 21 f36.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'search' NUM:'4' Confidence: 33.18%\n",
      "# 22 f37.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'backward' NUM:'0' Confidence: 42.63%\n",
      "# 23 f38.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'forward' NUM:'1' Confidence: 30.57%\n",
      "# 24 f39.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'forward' | 'NUM:1' Confidence: 73.26%\n",
      "# 25 f40.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'forward' NUM:'1' Confidence: 42.64%\n",
      "# 26 f71.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'grab' | 'NUM:2' Confidence: 90.10%\n",
      "# 27 f72.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'grab' | 'NUM:2' Confidence: 90.21%\n",
      "# 28 f73.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'grab' | 'NUM:2' Confidence: 98.75%\n",
      "# 29 g30.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'stop' | 'NUM:5' Confidence: 84.64%\n",
      "# 30 g31.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'forward' NUM:'1' Confidence: 52.07%\n",
      "# 31 g32.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 76.23%\n",
      "# 32 g33.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'stop' | 'NUM:5' Confidence: 85.27%\n",
      "# 33 g34.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'stop' NUM:'5' Confidence: 49.11%\n",
      "# 34 g35.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 98.84%\n",
      "# 35 g36.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'release' NUM:'3' Confidence: 39.07%\n",
      "# 36 g37.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'grab' NUM:'2' Confidence: 32.52%\n",
      "# 37 g38.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'search' | 'NUM:4' Confidence: 75.14%\n",
      "# 38 g39.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'search' NUM:'4' Confidence: 41.21%\n",
      "# 39 g40.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'backward' NUM:'0' Confidence: 24.38%\n",
      "# 40 g71.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'forward' | 'NUM:1' Confidence: 97.64%\n",
      "# 41 g72.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'stop' | 'NUM:5' Confidence: 91.49%\n",
      "# 42 g73.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'stop' | 'NUM:5' Confidence: 99.99%\n",
      "# 43 l30.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'grab' NUM:'2' Confidence: 51.59%\n",
      "# 44 l31.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnleft' | 'NUM:6' Confidence: 98.44%\n",
      "# 45 l32.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnleft' | 'NUM:6' Confidence: 77.46%\n",
      "# 46 l33.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnleft' | 'NUM:6' Confidence: 93.17%\n",
      "# 47 l34.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnleft' | 'NUM:6' Confidence: 99.12%\n",
      "# 48 l35.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'turnright' NUM:'7' Confidence: 46.04%\n",
      "# 49 l36.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnleft' | 'NUM:6' Confidence: 99.74%\n",
      "# 50 l37.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnleft' | 'NUM:6' Confidence: 93.97%\n",
      "# 51 l38.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnleft' | 'NUM:6' Confidence: 90.11%\n",
      "# 52 l39.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'grab' | 'NUM:2' Confidence: 90.35%\n",
      "# 53 l40.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'grab' NUM:'2' Confidence: 59.93%\n",
      "# 54 l71.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 89.77%\n",
      "# 55 l72.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 81.35%\n",
      "# 56 l73.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnleft' | 'NUM:6' Confidence: 99.92%\n",
      "# 57 r30.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnright' | 'NUM:7' Confidence: 99.65%\n",
      "# 58 r31.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnright' | 'NUM:7' Confidence: 73.70%\n",
      "# 59 r32.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnright' | 'NUM:7' Confidence: 98.01%\n",
      "# 60 r33.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnright' | 'NUM:7' Confidence: 99.18%\n",
      "# 61 r34.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnright' | 'NUM:7' Confidence: 97.36%\n",
      "# 62 r35.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnright' | 'NUM:7' Confidence: 99.87%\n",
      "# 63 r36.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'turnright' NUM:'7' Confidence: 65.06%\n",
      "# 64 r37.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'turnright' NUM:'7' Confidence: 59.02%\n",
      "# 65 r38.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnright' | 'NUM:7' Confidence: 75.06%\n",
      "# 66 r39.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnright' | 'NUM:7' Confidence: 96.68%\n",
      "# 67 r40.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnright' | 'NUM:7' Confidence: 89.05%\n",
      "# 68 r71.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnright' | 'NUM:7' Confidence: 99.86%\n",
      "# 69 r72.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnright' | 'NUM:7' Confidence: 98.72%\n",
      "# 70 r73.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnright' | 'NUM:7' Confidence: 99.99%\n",
      "# 71 re30.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 99.14%\n",
      "# 72 re31.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 99.94%\n",
      "# 73 re32.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 99.92%\n",
      "# 74 re33.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 99.94%\n",
      "# 75 re34.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 99.95%\n",
      "# 76 re35.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 99.85%\n",
      "# 77 re36.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 99.25%\n",
      "# 78 re37.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 97.78%\n",
      "# 79 re38.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 99.80%\n",
      "# 80 re39.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 99.01%\n",
      "# 81 re40.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 93.91%\n",
      "# 82 re72.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 99.75%\n",
      "# 83 re73.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 78.78%\n",
      "# 84 re74.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'release' | 'NUM:3' Confidence: 99.96%\n",
      "# 85 s30.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'stop' | 'NUM:5' Confidence: 99.39%\n",
      "# 86 s31.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'stop' | 'NUM:5' Confidence: 98.98%\n",
      "# 87 s32.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'stop' NUM:'5' Confidence: 55.12%\n",
      "# 88 s33.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'grab' | 'NUM:2' Confidence: 81.96%\n",
      "# 89 s34.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'search' NUM:'4' Confidence: 40.72%\n",
      "# 90 s35.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'grab' NUM:'2' Confidence: 68.49%\n",
      "# 91 s36.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'stop' NUM:'5' Confidence: 54.12%\n",
      "# 92 s37.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'release' NUM:'3' Confidence: 56.82%\n",
      "# 93 s38.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'release' NUM:'3' Confidence: 65.42%\n",
      "# 94 s39.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'stop' | 'NUM:5' Confidence: 99.92%\n",
      "# 95 s40.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'stop' | 'NUM:5' Confidence: 99.98%\n",
      "# 96 s71.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'turnright' | 'NUM:7' Confidence: 74.24%\n",
      "# 97 s72.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'turnright' NUM:'7' Confidence: 50.51%\n",
      "# 98 s73.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'stop' | 'NUM:5' Confidence: 100.00%\n",
      "# 99 se30.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'search' | 'NUM:4' Confidence: 99.98%\n",
      "# 100 se31.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'search' | 'NUM:4' Confidence: 99.97%\n",
      "# 101 se32.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'search' | 'NUM:4' Confidence: 98.75%\n",
      "# 102 se33.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'forward' | 'NUM:1' Confidence: 81.04%\n",
      "# 103 se34.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'search' | 'NUM:4' Confidence: 99.89%\n",
      "# 104 se35.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'search' | 'NUM:4' Confidence: 98.84%\n",
      "# 105 se36.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'search' | 'NUM:4' Confidence: 99.99%\n",
      "# 106 se37.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'search' | 'NUM:4' Confidence: 99.98%\n",
      "# 107 se38.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'search' | 'NUM:4' Confidence: 92.62%\n",
      "# 108 se39.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'search' | 'NUM:4' Confidence: 99.93%\n",
      "# 109 se40.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'search' | 'NUM:4' Confidence: 99.96%\n",
      "# 110 se71.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'search' NUM:'4' Confidence: 69.68%\n",
      "# 111 se72.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'OTHER_KEYWORDS' | ORIGINAL: 'grab' NUM:'2' Confidence: 52.28%\n",
      "# 112 se73.wav---------------------------------------------------\n",
      "INPUT SHAPE: (1, 32, 40, 1)\n",
      "Keyword Detected 'search' | 'NUM:4' Confidence: 82.27%\n"
     ]
    }
   ],
   "source": [
    "TEST_PATH = os.path.join(\"Data_Thai\",\"test\")\n",
    "pred_demo = Keyword_Demo_v2(model_path, DATASET_JSON, plot=False)\n",
    "file_test_list = [filenames for _,_,filenames in os.walk(TEST_PATH)][0]\n",
    "for idx, file in enumerate(file_test_list):\n",
    "    print(f\"# {idx+1} {file}---------------------------------------------------\")\n",
    "    AUDIO_DATA_INPUT = os.path.join(\"Data_Thai/test/\",file)\n",
    "    keyword_result = pred_demo.predict(AUDIO_DATA_INPUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_demo2 = Keyword_Demo_v2(model_path, DATASET_JSON, plot=False)\n",
    "for idx, (dirpath, dirnames, filenames) in enumerate(os.walk(DATASET_PATH)):\n",
    "    print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REAL-TIME SPEECH COMMAND RECOGNITION (INFERENCE PROCESS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUG - TEST PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vscode_audio import Audio\n",
    "x,sr = librosa.load(train_audio_sample, sr = 16000)\n",
    "x_augmented = random_shift(x)\n",
    "x_augmented = random_speed_up(x_augmented)\n",
    "x_augmented = random_change_pitch(x_augmented)\n",
    "Audio(x_augmented, sr)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "555a84b3913872e2e862f1e806eac0e56227ea6f84c49940b9c2017cb049ce4f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
